{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db1a2f17-0a6f-498a-8d3f-eb27606f79c0",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red\";> voice assistant</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8cd32f-d60d-4150-9f00-96191704cae1",
   "metadata": {},
   "source": [
    "<h1 style=\"color:Violet\";>importing libraries</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d278421-4148-45b4-ac3f-4abed846dc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c0259cfd-9138-4e62-81e4-7a40f6368fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703e0d86-5a15-4ebf-9477-bcb9ddac80bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6d3dc655-30af-4979-a128-06282c5c1ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from faster_whisper import WhisperModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a598cfca-89dc-4da1-9de1-4f85dfb049fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from silero_vad import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed1235a-f0a5-4828-8a00-b46c57a8e03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0caecca-1ef0-40bc-826d-9a498223b424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495f1bf0-3d19-43ed-8a25-f948e756c089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import edge_tts\n",
    "import nest_asyncio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b25b38-4b4e-4cd2-8ee5-c4acbb30ead8",
   "metadata": {},
   "source": [
    "<h1 style=color:\"Green\";> Voice Assistant</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d8f5785-54d2-4a60-8f57-2cb660b40edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to record audio\n",
    "def record_audio(duration, samplerate=16000):\n",
    "    print(f\"Recording for {duration} seconds...\")\n",
    "    audio_data = sd.rec(int(duration * samplerate), samplerate=samplerate, channels=1, dtype=np.float32)\n",
    "    sd.wait()  # Wait until recording is finished\n",
    "    print(\"Recording finished.\")\n",
    "    return audio_data.flatten()  # Return 1D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f3b61d4-cee7-4655-a9e6-34fdc8bc8cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to chunk audio data\n",
    "def chunk_audio(audio_data, chunk_size):\n",
    "    for i in range(0, len(audio_data), chunk_size):\n",
    "        yield audio_data[i:i + chunk_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f132050c-e64d-4523-847a-6a105cb2dc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikil\\Downloads\\ml projects\\prj7\\voice_assistant\\Lib\\site-packages\\whisper\\__init__.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording for 10 seconds...\n",
      "Recording finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikil\\Downloads\\ml projects\\prj7\\voice_assistant\\Lib\\site-packages\\whisper\\transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded  Text:  Thank you.  Thank you.  Thank you. Thank you.  Thank you.  Thank you.  Thank you.  Thank you.  Thank you.  Thank you.\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import whisper\n",
    "\n",
    "# Initialize Whisper model\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# Define the duration of the audio recording in seconds\n",
    "duration = 10\n",
    "\n",
    "# Define the sample rate\n",
    "samplerate = 16000\n",
    "\n",
    "# Define the chunk size for processing\n",
    "chunk_size = 16000\n",
    "\n",
    "# Define the VAD model and threshold\n",
    "# For this example, we'll use a simple threshold-based VAD\n",
    "def simple_vad(audio, threshold=0.5):\n",
    "    # Calculate the energy of the audio signal\n",
    "    energy = np.sum(np.abs(audio) ** 2)\n",
    "    # If the energy is above the threshold, consider it speech\n",
    "    if energy > threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "vad_model = simple_vad\n",
    "threshold = 0.5\n",
    "\n",
    "# Define a list to store the full text output\n",
    "full_text_output = []\n",
    "\n",
    "def record_audio(duration, samplerate=16000):\n",
    "    print(f\"Recording for {duration} seconds...\")\n",
    "    audio_data = sd.rec(int(duration * samplerate), samplerate=samplerate, channels=1, dtype=np.float32)\n",
    "    sd.wait()  # Wait until recording is finished\n",
    "    print(\"Recording finished.\")\n",
    "    return audio_data.flatten()\n",
    "\n",
    "def chunk_audio(audio_data, chunk_size):\n",
    "    for i in range(0, len(audio_data), chunk_size):\n",
    "        yield audio_data[i:i + chunk_size]\n",
    "\n",
    "def get_speech_ts(audio, vad_model, threshold=0.5):\n",
    "    # Implement your VAD logic here to detect speech segments\n",
    "    # Return a list of dictionaries with 'start' and 'end' timestamps\n",
    "    speech_segments = []\n",
    "    if vad_model(audio, threshold):\n",
    "        speech_segments.append({'start': 0, 'end': len(audio) / samplerate})\n",
    "    return speech_segments\n",
    "\n",
    "# Record audio\n",
    "audio_data = record_audio(duration)\n",
    "\n",
    "# Iterate over audio chunks and process each one\n",
    "for chunk in chunk_audio(audio_data, chunk_size):\n",
    "    # Detect speech segments within the chunk\n",
    "    speech_segments = get_speech_ts(chunk, vad_model, threshold=0.5)\n",
    "\n",
    "    if speech_segments is not None and len(speech_segments) > 0:\n",
    "        # Process each detected speech segment\n",
    "        for seg in speech_segments:\n",
    "            start_sample = int(seg['start'] * samplerate)\n",
    "            end_sample = int(seg['end'] * samplerate)\n",
    "            speech_audio = chunk[start_sample:end_sample]\n",
    "            \n",
    "            # Convert speech audio to text using Whisper\n",
    "            result = model.transcribe(speech_audio)\n",
    "            text_output = result[\"text\"]\n",
    "            full_text_output.append(text_output)\n",
    "\n",
    "# Join all the transcribed chunks into final text\n",
    "final_transcription = \" \".join(full_text_output)\n",
    "\n",
    "print(f\"Recorded  Text: {final_transcription}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a146e6d4-2100-49af-9262-d40b9f957c65",
   "metadata": {},
   "source": [
    "<h1 style = \"color:Green\";>LLM  working </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aad33333-abde-40e3-bf36-0887a30cdb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: Tell me the basics of Machine learning?\n",
      "\n",
      "Machine learning is a new field of research that has been around for a while. It is a new field of research that has been around for a while. It is a new field of research that has\n",
      "Audio saved to output_audio.wav\n",
      "Audio file created successfully. Check your media player.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import asyncio\n",
    "import edge_tts\n",
    "import nest_asyncio\n",
    "import os\n",
    "\n",
    "# Apply nest_asyncio to handle asyncio in Jupyter notebooks\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def text_to_speech(text, output_file, voice=\"en-US-JennyNeural\", rate=\"+0%\", pitch=\"+0Hz\"):\n",
    "    # Create a TTS engine with tunable parameters\n",
    "    communicate = edge_tts.Communicate(text, voice=voice, rate=rate, pitch=pitch)\n",
    "    await communicate.save(output_file)\n",
    "    print(f\"Audio saved to {output_file}\")\n",
    "\n",
    "# Define or set the input text\n",
    "text_output = \"Tell me the basics of Machine learning?\"\n",
    "\n",
    "# Load pre-trained GPT-2 model\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Encode the input text\n",
    "inputs = tokenizer.encode(text_output, return_tensors=\"pt\")\n",
    "\n",
    "# Set pad_token_id to eos_token_id for open-end generation\n",
    "pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Generate output, limiting the response to 50 tokens\n",
    "output_tokens = model.generate(\n",
    "    inputs,\n",
    "    max_length=50,\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=pad_token_id\n",
    ")\n",
    "\n",
    "# Decode the response\n",
    "output_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "print(f\"LLM Response: {output_text}\")\n",
    "\n",
    "# Convert the LLM response text to speech with custom pitch and rate\n",
    "output_file = \"output_audio.wav\"\n",
    "asyncio.run(text_to_speech(output_text, output_file, voice=\"en-US-JennyNeural\", rate=\"-10%\", pitch=\"+2Hz\"))\n",
    "\n",
    "# Check if the audio file was created and is not empty\n",
    "if os.path.exists(output_file) and os.path.getsize(output_file) > 0:\n",
    "    print(\"Audio file created successfully. Check your media player.\")\n",
    "else:\n",
    "    print(\"Failed to create audio file or the file is empty.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f08ef3a-1736-4025-97e9-c9d8d02486b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_kernel",
   "language": "python",
   "name": "torch_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
